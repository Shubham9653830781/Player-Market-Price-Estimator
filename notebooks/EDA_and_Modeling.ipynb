{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Player Market Price Estimator \u2014 EDA, Training & SHAP\nThis notebook runs the full pipeline interactively: EDA \u2192 Feature engineering \u2192 Train (RF & XGB) \u2192 Evaluate \u2192 SHAP explainability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport shap\npd.set_option('display.max_columns', None)\n%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/processed/players_processed.csv')\nprint('Rows:', len(df))\ndf.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick data summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(df.describe(include='all'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Market value distribution (original and log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nsns.histplot(df['market_value_num'], bins=40)\nplt.title('market_value (numeric)')\nplt.subplot(1,2,2)\nsns.histplot(df['target'], bins=40)\nplt.title('log1p(market_value) target')\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,6))\nsns.heatmap(df[['age','goals_per_90','assists_per_90','contract_years_left','market_value_num']].corr(), annot=True)\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature engineering (age^2, is_young)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['age_sq'] = df['age']**2\ndf['is_young'] = (df['age'] < 23).astype(int)\ndf[['age','age_sq','is_young']].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train/Test split and feature set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = ['age','age_sq','goals_per_90','assists_per_90','contract_years_left','club_rank_pct']\ncat = ['pos_bucket','nationality']\nX = df[features + cat]\ny = df['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint('Train:', len(X_train), 'Test:', len(X_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing: scaling numeric and one-hot encoding small categorical features, target encoding for high-cardinality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom category_encoders import TargetEncoder\nnum_features = features\nhigh_card = ['nationality']\ncat_small = ['pos_bucket']\npreprocessor = ColumnTransformer([\n    ('num', StandardScaler(), num_features),\n    ('te', TargetEncoder(), high_card),\n    ('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), cat_small)\n], remainder='drop')\n# fit transformer quickly\npreprocessor.fit(X_train, y_train)\nX_train_p = preprocessor.transform(X_train)\nX_test_p = preprocessor.transform(X_test)\nprint('Processed shapes:', X_train_p.shape, X_test_p.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Random Forest (on preprocessed features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf = RandomForestRegressor(n_estimators=150, max_depth=12, random_state=42, n_jobs=-1)\nrf.fit(X_train_p, y_train)\nrf_preds = rf.predict(X_test_p)\nprint('RF RMSE:', mean_squared_error(y_test, rf_preds, squared=False))\nprint('RF R2:', r2_score(y_test, rf_preds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train XGBoost (on preprocessed features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb = XGBRegressor(n_estimators=200, max_depth=6, learning_rate=0.05, random_state=42, verbosity=0)\n# XGBoost accepts numpy arrays\nxgb.fit(X_train_p, y_train, eval_set=[(X_test_p, y_test)], early_stopping_rounds=20, verbose=False)\nxgb_preds = xgb.predict(X_test_p)\nprint('XGB RMSE:', mean_squared_error(y_test, xgb_preds, squared=False))\nprint('XGB R2:', r2_score(y_test, xgb_preds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save best model and preprocessor (pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n# choose better model by RMSE\nrf_rmse = mean_squared_error(y_test, rf_preds, squared=False)\nxgb_rmse = mean_squared_error(y_test, xgb_preds, squared=False)\nif xgb_rmse < rf_rmse:\n    best = ('xgb', xgb)\nelse:\n    best = ('rf', rf)\n# save preprocessor and model\njoblib.dump(preprocessor, 'models/preprocessor.pkl')\njoblib.dump(best[1], f\"models/best_model_{best[0]}.pkl\")\nprint('Saved model:', best[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SHAP explanation for the best model (XGBoost or RF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = joblib.load(f\"models/best_model_{best[0]}.pkl\")\n# For RF and XGB, TreeExplainer works\nexplainer = shap.TreeExplainer(model)\n# Use a small subset for SHAP\nX_shap = X_test_p[:200]\nshap_values = explainer.shap_values(X_shap)\n# summary plot\nshap.summary_plot(shap_values, X_shap)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final: Show a few original-valued comparisons (convert back from log1p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "preds_orig = np.expm1(xgb_preds if best[0]=='xgb' else rf_preds)\nactual_orig = np.expm1(y_test.values)\ncomp = pd.DataFrame({'pred':preds_orig[:10], 'actual': actual_orig[:10]})\ncomp\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}